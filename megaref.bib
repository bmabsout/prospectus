@article{10.1145/325165.325247,
	title        = {An Image Synthesizer},
	author       = {Perlin, Ken},
	year         = 1985,
	month        = jul,
	journal      = {SIGGRAPH Comput. Graph.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 19,
	number       = 3,
	pages        = {287–296},
	doi          = {10.1145/325165.325247},
	issn         = {0097-8930},
	issue_date   = {July 1985},
	numpages     = 10,
	keywords     = {pixel stream editor, waves, functional composition, solid texture, stochastic modelling, turbulence, algorithm development, interactive, space function, fire}
}
@inproceedings{10.1145/325334.325247,
	title        = {An Image Synthesizer},
	author       = {Perlin, Ken},
	year         = 1985,
	booktitle    = {Proceedings of the 12th Annual Conference on Computer Graphics and Interactive Techniques},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGGRAPH ’85},
	pages        = {287–296},
	doi          = {10.1145/325334.325247},
	isbn         = {0897911660},
	numpages     = 10,
	keywords     = {space function, stochastic modelling, waves, fire, pixel stream editor, turbulence, solid texture, functional composition, interactive, algorithm development}
}
@inproceedings{6025669,
	title        = {Reinforcement learning model, algorithms and its application},
	author       = {Qiang, Wang and Zhongli, Zhan},
	year         = 2011,
	booktitle    = {2011 International Conference on Mechatronic Science, Electric Engineering and Computer (MEC)},
	volume       = {},
	number       = {},
	pages        = {1143--1146},
	doi          = {10.1109/MEC.2011.6025669},
	keywords     = {Learning;Machine learning;Function approximation;Algorithm design and analysis;Heuristic algorithms;Intelligent agents;Learning systems;Reinforcement Learning;Sarsa;temporal difference;Q-learning;function approximation}
}
@inproceedings{6252825,
	title        = {Improving wet clutch engagement with reinforcement learning},
	author       = {Van Vaerenbergh, Kevin and Rodríguez, Abdel and Gagliolo, Matteo and Vrancx, Peter and Nowé, Ann and Stoev, Julian and Goossens, Stijn and Pinte, Gregory and Symens, Wim},
	year         = 2012,
	booktitle    = {The 2012 International Joint Conference on Neural Networks (IJCNN)},
	volume       = {},
	number       = {},
	pages        = {1--8},
	doi          = {10.1109/IJCNN.2012.6252825},
	keywords     = {Pistons;Torque;Shafts;Sensors;Friction;Noise measurement;Trajectory}
}
@inproceedings{6784622,
	title        = {On the Behaviour of Scalarization Methods for the Engagement of a Wet Clutch},
	author       = {Brys, Tim and Moffaert, Kristof Van and Vaerenbergh, Kevin Van and Nowé, Ann},
	year         = 2013,
	booktitle    = {2013 12th International Conference on Machine Learning and Applications},
	volume       = 1,
	number       = {},
	pages        = {258--263},
	doi          = {10.1109/ICMLA.2013.52},
	keywords     = {Chebyshev approximation;Pistons;Learning (artificial intelligence);Friction;Optimization;Shafts;Torque;Multi-objective;reinforcement learning;scalarization}
}
@inproceedings{A2C,
	title        = {Asynchronous methods for deep reinforcement learning},
	author       = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	year         = 2016,
	booktitle    = {International conference on machine learning},
	pages        = {1928--1937},
	organization = {PMLR}
}
@misc{A3C,
	title        = {Asynchronous Advantage Actor Critic: Non-asymptotic Analysis and Linear Speedup},
	author       = {Han Shen and Kaiqing Zhang and Mingyi Hong and Tianyi Chen},
	year         = 2020,
	eprint       = {2012.15511},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{abadi2016tensorflow,
	title        = {Tensorflow: Large-scale machine learning on heterogeneous distributed systems},
	author       = {Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1603.04467}
}
@article{adaptiveRLAerial,
	title        = {Deep Reinforcement Learning-Based Adaptive Controller for Trajectory Tracking and Altitude Control of an Aerial Robot},
	author       = {Barzegar, Ali and Lee, Deokjin},
	year         = 2022,
	month        = {05},
	journal      = {Applied Sciences},
	volume       = 12,
	pages        = 4764,
	doi          = {10.3390/app12094764}
}
@article{alegre2023sample,
	title        = {Sample-efficient multi-objective learning via generalized policy improvement prioritization},
	author       = {Alegre, Lucas N and Bazzan, Ana LC and Roijers, Diederik M and Now{\'e}, Ann and da Silva, Bruno C},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2301.07784}
}
@article{alonsodeepRLAAANav20,
	title        = {Deep Reinforcement Learning for Navigation in AAA Video Games},
	author       = {Alonso, Eloi and Pete, Maxim and Goumard, David and Romoff, Joshua},
	year         = 2020,
	journal      = {NeuriPS Workshop on Challenges of Real-World RL}
}
@article{alphago,
	title        = {Mastering the game of Go with deep neural networks and tree search},
	author       = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
	year         = 2016,
	journal      = {nature},
	publisher    = {Nature Publishing Group},
	volume       = 529,
	number       = 7587,
	pages        = 484
}
@article{alphastar,
	title        = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
	author       = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
	year         = 2019,
	journal      = {Nature},
	publisher    = {Nature Publishing Group},
	volume       = 575,
	number       = 7782,
	pages        = {350--354}
}
@article{alphazero,
	title        = {Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
	author       = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1712.01815}
}
@misc{amodei2016concrete,
	title        = {Concrete Problems in AI Safety},
	author       = {Dario Amodei and Chris Olah and Jacob Steinhardt and Paul Christiano and John Schulman and Dan Mané},
	year         = 2016,
	eprint       = {1606.06565},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@misc{anchor-github,
	title        = {Anchor Critics Homepage},
	url          = {https://github.com/bmabsout/AnchoredActorCritic}
}
@inproceedings{andrychowicz2021what,
	title        = {What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study},
	author       = {Marcin Andrychowicz and Anton Raichuk and Piotr Sta{\'n}czyk and Manu Orsini and Sertan Girgin and Rapha{\"e}l Marinier and Leonard Hussenot and Matthieu Geist and Olivier Pietquin and Marcin Michalski and Sylvain Gelly and Olivier Bachem},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations}
}
@misc{anonymous,
	title        = {Suppressed for Anonymity},
	author       = {Author, N. N.},
	year         = 2021
}
@article{autotesting,
	title        = {Automated Video Game Testing Using Synthetic and Humanlike Agents},
	author       = {S. {Ariyurek} and A. {Betin-Can} and E. {Surer}},
	year         = 2021,
	journal      = {IEEE Transactions on Games},
	volume       = 13,
	number       = 1,
	pages        = {50--67},
	doi          = {10.1109/TG.2019.2947597}
}
@inproceedings{barto2017some,
	title        = {Some recent applications of reinforcement learning},
	author       = {Barto, Andrew G and Thomas, Philip S and Sutton, Richard S},
	year         = 2017,
	booktitle    = {Proceedings of the Eighteenth Yale Workshop on Adaptive and Learning Systems}
}
@misc{baselines,
	title        = {OpenAI Baselines},
	author       = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
	year         = 2017,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/openai/baselines}}
}
@inproceedings{Bayesian_Optimization,
	title        = {Robust Model-free Reinforcement Learning with Multi-objective Bayesian Optimization},
	author       = {Turchetta, Matteo and Krause, Andreas and Trimpe, Sebastian},
	year         = 2020,
	booktitle    = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
	volume       = {},
	number       = {},
	pages        = {10702--10708},
	doi          = {10.1109/ICRA40945.2020.9197000},
	keywords     = {Robustness;Optimization;Training;Control theory;Computational modeling;Learning (artificial intelligence);Bayes methods}
}
@inproceedings{bechtel2018deeppicar,
	title        = {Deeppicar: A low-cost deep neural network-based autonomous car},
	author       = {Bechtel, Michael G and McEllhiney, Elise and Kim, Minje and Yun, Heechul},
	year         = 2018,
	booktitle    = {2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)},
	pages        = {11--21},
	organization = {IEEE}
}
@inproceedings{Belta_Temporal,
	title        = {Reinforcement learning with temporal logic rewards},
	author       = {Li, Xiao and Vasile, Cristian-Ioan and Belta, Calin},
	year         = 2017,
	booktitle    = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	pages        = {3834--3839},
	organization = {IEEE}
}
@inproceedings{benchmarkingRL,
	title        = {Benchmarking Deep Reinforcement Learning for Continuous Control},
	author       = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
	year         = 2016,
	booktitle    = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
	location     = {New York, NY, USA},
	publisher    = {JMLR.org},
	series       = {ICML'16},
	pages        = {1329--1338},
	numpages     = 10,
	acmid        = 3045531
}
@article{benchmarkingRobo,
	title        = {Benchmarking Reinforcement Learning Algorithms on Real-World Robots},
	author       = {A. Rupam Mahmood and Dmytro Korenkevych and Gautham Vasan and William Ma and James Bergstra},
	year         = 2018,
	journal      = {Conference on Robot Learning},
	url          = {http://arxiv.org/abs/1809.07731},
	archiveprefix = {arXiv},
	eprint       = {1809.07731},
	timestamp    = {Fri, 05 Oct 2018 11:34:52 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1809-07731.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{betaflight-homepage,
	title        = {Betaflight},
	url          = {https://betaflight.com/}
}
@inproceedings{bozhinoski2016leveraging,
	title        = {Leveraging collective run-time adaptation for uav-based systems},
	author       = {Bozhinoski, Darko and Bucchiarone, Antonio and Malavolta, Ivano and Marconi, Annapaola and Pelliccione, Patrizio},
	year         = 2016,
	booktitle    = {SEAA},
	pages        = {214--221},
	organization = {IEEE}
}
@article{bryce2007probabilistic,
	title        = {Probabilistic planning is multi-objective},
	author       = {Bryce, Daniel and Cushing, William and Kambhampati, Subbarao},
	year         = 2007,
	journal      = {ASU CSE TR},
	pages        = {07--006}
}
@article{castelletti2013multiobjective,
	title        = {A multiobjective reinforcement learning approach to water resources systems operation: Pareto frontier approximation in a single run},
	author       = {Castelletti, Andrea and Pianosi, Francesca and Restelli, Marcello},
	year         = 2013,
	journal      = {Water Resources Research},
	publisher    = {Wiley Online Library},
	volume       = 49,
	number       = 6,
	pages        = {3476--3486}
}
@inproceedings{catastrophic-forgetting-binici,
	title        = {Preventing Catastrophic Forgetting and Distribution Mismatch in Knowledge Distillation via Synthetic Data},
	author       = {Binici, Kuluhan and Trung Pham, Nam and Mitra, Tulika and Leman, Karianto},
	year         = 2022,
	booktitle    = {2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
	volume       = {},
	number       = {},
	pages        = {3625--3633},
	doi          = {10.1109/WACV51458.2022.00368},
	keywords     = {Deep learning;Energy consumption;Computer vision;Computational modeling;Neural networks;Memory management;Training data;Deep Learning -> Efficient Training and Inference Methods for Networks Computational Photography; Image and Video Synthesis}
}
@misc{catastrophic-forgetting-wolczyk,
	title        = {Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem},
	author       = {Wolczyk,  Maciej and Cupial,  Bartlomiej and Ostaszewski,  Mateusz and Bortkiewicz,  Michal and Zajac,  Michal and Pascanu,  Razvan and Kucinski,  Lukasz and Milos,  Piotr},
	year         = 2024,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2402.02868},
	url          = {https://arxiv.org/abs/2402.02868},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences}
}
@article{chen2021randomized,
	title        = {Randomized ensembled double q-learning: Learning fast without a model},
	author       = {Chen, Xinyue and Wang, Che and Zhou, Zijian and Ross, Keith},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2101.05982}
}
@article{Cheng_Orosz_Murray_Burdick_2019,
	title        = {End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks},
	author       = {Cheng, Richard and Orosz, Gábor and Murray, Richard M. and Burdick, Joel W.},
	year         = 2019,
	month        = {Jul.},
	journal      = {Proc. AAAI Conf. Artif. Intell.},
	volume       = 33,
	number       = {01},
	pages        = {3387--3395},
	doi          = {10.1609/aaai.v33i01.33013387},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/4213}
}
@article{Chinchali2021,
	title        = {Network offloading policies for cloud robotics: a learning-based approach},
	author       = {Chinchali,  Sandeep and Sharma,  Apoorva and Harrison,  James and Elhafsi,  Amine and Kang,  Daniel and Pergament,  Evgenya and Cidon,  Eyal and Katti,  Sachin and Pavone,  Marco},
	year         = 2021,
	month        = jul,
	journal      = {Autonomous Robots},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 45,
	number       = 7,
	pages        = {997–1012},
	doi          = {10.1007/s10514-021-09987-4},
	issn         = {1573-7527},
	url          = {http://dx.doi.org/10.1007/s10514-021-09987-4}
}
@inproceedings{cisse2017parseval,
	title        = {Parseval networks: Improving robustness to adversarial examples},
	author       = {Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
	year         = 2017,
	booktitle    = {International Conference on Machine Learning},
	pages        = {854--863},
	organization = {PMLR}
}
@inproceedings{climbing,
	title        = {A Reinforcement Learning Approach To Synthesizing Climbing Movements},
	author       = {K. {Naderi} and A. {Babadi} and S. {Roohi} and P. {Hämäläinen}},
	year         = 2019,
	booktitle    = {2019 IEEE Conference on Games (CoG)},
	volume       = {},
	number       = {},
	pages        = {1--7},
	doi          = {10.1109/CIG.2019.8848127}
}
@article{DART,
	title        = {DART: Dynamic Animation and Robotics Toolkit},
	author       = {Jeongseok Lee and Michael Grey and Sehoon Ha and Tobias Kunz and Sumit Jain and Yuting Ye and Siddhartha Srinivasa and Mike Stilman and C. Liu},
	year         = 2018,
	journal      = {Journal of Open Source Software},
	publisher    = {The Open Journal},
	volume       = 3,
	number       = 22,
	pages        = 500,
	doi          = {10.21105/joss.00500}
}
@article{DDPG,
	title        = {Continuous control with deep reinforcement learning},
	author       = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
	year         = 2016,
	journal      = {International Conference on Learning Representations},
	archiveprefix = {arXiv},
	eprint       = {1509.02971},
	timestamp    = {Mon, 13 Aug 2018 16:46:11 +0200},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{de2011flexible,
	title        = {Flexible data-centric UAV platform eases mission adaptation. White paper, Available online},
	author       = {de Jong, E},
	year         = 2011
}
@article{Deb2006ReferencePB,
	title        = {Reference point based multi-objective optimization using evolutionary algorithms},
	author       = {Kalyanmoy Deb and J. Sundar and Udaya Bhaskara and Rao N and Shamik Chaudhuri},
	year         = 2006,
	journal      = {Proceedings of the 8th annual conference on Genetic and evolutionary computation},
	url          = {https://api.semanticscholar.org/CorpusID:6802382}
}
@article{Degrave2022,
	title        = {Magnetic control of tokamak plasmas through deep reinforcement learning},
	author       = {Degrave,  Jonas and Felici,  Federico and Buchli,  Jonas and Neunert,  Michael and Tracey,  Brendan and Carpanese,  Francesco and Ewalds,  Timo and Hafner,  Roland and Abdolmaleki,  Abbas and de las Casas,  Diego and Donner,  Craig and Fritz,  Leslie and Galperti,  Cristian and Huber,  Andrea and Keeling,  James and Tsimpoukelli,  Maria and Kay,  Jackie and Merle,  Antoine and Moret,  Jean-Marc and Noury,  Seb and Pesamosca,  Federico and Pfau,  David and Sauter,  Olivier and Sommariva,  Cristian and Coda,  Stefano and Duval,  Basil and Fasoli,  Ambrogio and Kohli,  Pushmeet and Kavukcuoglu,  Koray and Hassabis,  Demis and Riedmiller,  Martin},
	year         = 2022,
	month        = feb,
	journal      = {Nature},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 602,
	number       = 7897,
	pages        = {414–419},
	doi          = {10.1038/s41586-021-04301-9},
	issn         = {1476-4687},
	url          = {http://dx.doi.org/10.1038/s41586-021-04301-9}
}
@article{Dextrous,
	title        = {Learning dexterous in-hand manipulation},
	author       = {OpenAI: Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Józefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
	year         = 2020,
	journal      = {The International Journal of Robotics Research},
	volume       = 39,
	number       = 1,
	pages        = {3--20},
	doi          = {10.1177/0278364919887447},
	url          = {https://doi.org/10.1177/0278364919887447},
	eprint       = {https://doi.org/10.1177/0278364919887447},
	abstract     = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM.}
}
@inproceedings{double_q_learning,
	title        = {Deep reinforcement learning with double Q-Learning},
	author       = {Hasselt, Hado van and Guez, Arthur and Silver, David},
	year         = 2016,
	booktitle    = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
	location     = {Phoenix, Arizona},
	publisher    = {AAAI Press},
	series       = {AAAI'16},
	pages        = {2094–2100},
	numpages     = 7
}
@incollection{DoubleQ,
	title        = {Double Q-learning},
	author       = {Hado V. Hasselt},
	year         = 2010,
	booktitle    = {Advances in Neural Information Processing Systems 23},
	publisher    = {Curran Associates, Inc.},
	pages        = {2613--2621},
	editor       = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta}
}
@article{dqn,
	title        = {Playing atari with deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1312.5602}
}
@article{DQN_paper,
	title        = {Human-level control through deep reinforcement learning},
	author       = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and Andreas Kirkeby Fidjeland and Georg Ostrovski and Stig Petersen and Charlie Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
	year         = 2015,
	journal      = {Nature},
	volume       = 518,
	pages        = {529--533},
	url          = {https://api.semanticscholar.org/CorpusID:205242740}
}
@article{DRL_survey,
	title        = {Deep reinforcement learning: a survey},
	author       = {Wang, Hao-nan and Liu, Ning and Zhang, Yi-yun and Feng, Da-wei and Huang, Feng and Li, Dong-sheng and Zhang, Yi-ming},
	year         = 2020,
	journal      = {Frontiers of Information Technology \& Electronic Engineering},
	publisher    = {Springer},
	volume       = 21,
	number       = 12,
	pages        = {1726--1744}
}
@book{DSP,
	title        = {Digital Signal Processing (3rd Ed.): Principles, Algorithms, and Applications},
	author       = {Proakis, John G. and Manolakis, Dimitris G.},
	year         = 1996,
	publisher    = {Prentice-Hall, Inc.},
	address      = {USA},
	isbn         = {0133737624}
}
@book{DudaHart2nd,
	title        = {Pattern Classification},
	author       = {R. O. Duda and P. E. Hart and D. G. Stork},
	year         = 2000,
	publisher    = {John Wiley and Sons},
	edition      = {2nd}
}
@article{dynamicweights,
	title        = {Dynamic weights in multi-objective deep reinforcement learning},
	author       = {Abels, Axel and Roijers, Diederik M and Lenaerts, Tom and Now{\'e}, Ann and Steckelmacher, Denis},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1809.07803}
}
@inproceedings{Engstrom2020Implementation,
	title        = {Implementation Matters in Deep RL: A Case Study on PPO and TRPO},
	author       = {Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{eysenbach2018diversity,
	title        = {Diversity is All You Need: Learning Skills without a Reward Function},
	author       = {Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},
	year         = 2019,
	booktitle    = {ICLR},
	url          = {https://openreview.net/forum?id=SJx63jRqFm}
}
@article{Fleming1986HowNT,
	title        = {How not to lie with statistics: the correct way to summarize benchmark results},
	author       = {Philip J. Fleming and John J. Wallace},
	year         = 1986,
	journal      = {Commun. ACM},
	volume       = 29,
	pages        = {218--221}
}
@article{FU2022104165,
	title        = {Applications of reinforcement learning for building energy efficiency control: A review},
	author       = {Qiming Fu and Zhicong Han and Jianping Chen and You Lu and Hongjie Wu and Yunzhe Wang},
	year         = 2022,
	journal      = {Journal of Building Engineering},
	volume       = 50,
	pages        = 104165,
	doi          = {https://doi.org/10.1016/j.jobe.2022.104165},
	issn         = {2352-7102},
	url          = {https://www.sciencedirect.com/science/article/pii/S2352710222001784},
	keywords     = {Reinforcement learning, Intelligent buildings, Energy consumption}
}
@inproceedings{gazebo,
	title        = {Design and use paradigms for gazebo, an open-source multi-robot simulator},
	author       = {Koenig, Nathan and Howard, Andrew},
	booktitle    = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No. 04CH37566)},
	volume       = 3,
	pages        = {2149--2154},
	organization = {IEEE}
}
@inproceedings{golemo2018sim,
	title        = {Sim-to-real transfer with neural-augmented robot simulation},
	author       = {Golemo, Florian and Taiga, Adrien Ali and Courville, Aaron and Oudeyer, Pierre-Yves},
	year         = 2018,
	booktitle    = {CoRL},
	pages        = {817--828},
	organization = {PMLR}
}
@article{graves2012long,
	title        = {Long short-term memory},
	author       = {Graves, Alex and Graves, Alex},
	year         = 2012,
	journal      = {Supervised sequence labelling with recurrent neural networks},
	publisher    = {Springer},
	pages        = {37--45}
}
@article{GYM,
	title        = {OpenAI Gym},
	author       = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1606.01540},
	archiveprefix = {arXiv},
	eprint       = {1606.01540},
	timestamp    = {Mon, 13 Aug 2018 16:48:42 +0200}
}
@inbook{Hajek1998,
	title        = {Product Logic, G{\"o}del Logic (and Boolean Logic)},
	author       = {H{\'a}jek, Petr},
	year         = 1998,
	booktitle    = {Metamathematics of Fuzzy Logic},
	publisher    = {Springer Netherlands},
	address      = {Dordrecht},
	pages        = {89--107},
	doi          = {10.1007/978-94-011-5300-3_4},
	isbn         = {978-94-011-5300-3},
	url          = {https://doi.org/10.1007/978-94-011-5300-3_4},
	abstract     = {We are going to investigate the second of the three most important prepositional calculi, namely PC(*II) where *II is the product t-norm; we shall call this logic just the product logic and denote it by II. Recall that the corresponding implication is Goguen and the corresponding negation is G{\"o}del negation (cf. 2.1.11,2.1.17). We present an axiom system (extension of BL by two axioms) and show its completeness21 by relating linearly ordered II-algebras --- called product algebras --- to ordered Abelian groups, similarly as in the case of MV-algebras (but now the proof is much simpler than in the case of MV-algebras). We also show that {\L}ukasiewicz logic {\L} has a faithful interpretation in II.22 We shall close the section by discussing some additional topics. Convention: In this section {\textrightarrow} (without any subscript) will be Goguen implication; the product conjunction will be denoted by ⊙.}
}
@article{Han2016DeepCC,
	title        = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
	author       = {Song Han and Huizi Mao and W. Dally},
	year         = 2016,
	journal      = {arXiv: Computer Vision and Pattern Recognition}
}
@inproceedings{hasani2021liquid,
	title        = {Liquid time-constant networks},
	author       = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
	year         = 2021,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 35,
	number       = 9,
	pages        = {7657--7666}
}
@article{hasani2022closed,
	title        = {Closed-form continuous-time neural networks},
	author       = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Liebenwein, Lucas and Ray, Aaron and Tschaikowski, Max and Teschl, Gerald and Rus, Daniela},
	year         = 2022,
	journal      = {Nature Machine Intelligence},
	publisher    = {Nature Publishing Group UK London},
	volume       = 4,
	number       = 11,
	pages        = {992--1003}
}
@inproceedings{hayes2023brief,
	title        = {A Brief Guide to Multi-Objective Reinforcement Learning and Planning},
	author       = {Hayes, Conor F and R{\u{a}}dulescu, Roxana and Bargiacchi, Eugenio and Kallstrom, Johan and Macfarlane, Matthew and Reymond, Mathieu and Verstraeten, Timothy and Zintgraf, Luisa M and Dazeley, Richard and Heintz, Fredrik and others},
	year         = 2023,
	booktitle    = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
	pages        = {1988--1990}
}
@article{hebbian-compliant-control,
	title        = {Reward-Modulated Hebbian Plasticity as Leverage for Partially Embodied Control in Compliant Robotics},
	author       = {Burms, Jeroen  and Caluwaerts, Ken  and Dambre, Joni},
	year         = 2015,
	journal      = {Frontiers in Neurorobotics},
	volume       = 9,
	doi          = {10.3389/fnbot.2015.00009},
	issn         = {1662-5218},
	url          = {https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2015.00009},
	abstract     = {<p>In embodied computation (or morphological computation), part of the complexity of motor control is offloaded to the body dynamics. We demonstrate that a simple Hebbian-like learning rule can be used to train systems with (partial) embodiment, and can be extended outside of the scope of traditional neural networks. To this end, we apply the learning rule to optimize the connection weights of recurrent neural networks with different topologies and for various tasks. We then apply this learning rule to a simulated compliant tensegrity robot by optimizing static feedback controllers that directly exploit the dynamics of the robot body. This leads to partially embodied controllers, i.e., hybrid controllers that naturally integrate the computations that are performed by the robot body into a neural network architecture. Our results demonstrate the universal applicability of reward-modulated Hebbian learning. Furthermore, they demonstrate the robustness of systems trained with the learning rule. This study strengthens our belief that compliant robots should or can be seen as computational units, instead of dumb hardware that needs a complex controller. This link between compliant robotics and neural networks is also the main reason for our search for simple universal learning rules for both neural networks and robotics.</p>}
}
@article{hebbian-handshake,
	title        = {Hebbian Plasticity in CPG Controllers Facilitates Self-Synchronization for Human-Robot Handshaking},
	author       = {Jouaiti, Melanie  and Caron, Lancelot  and Hénaff, Patrick},
	year         = 2018,
	journal      = {Frontiers in Neurorobotics},
	volume       = 12,
	doi          = {10.3389/fnbot.2018.00029},
	issn         = {1662-5218},
	url          = {https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2018.00029},
	abstract     = {<p>It is well-known that human social interactions generate synchrony phenomena which are often unconscious. If the interaction between individuals is based on rhythmic movements, synchronized and coordinated movements will emerge from the social synchrony. This paper proposes a plausible model of plastic neural controllers that allows the emergence of synchronized movements in physical and rhythmical interactions. The controller is designed with central pattern generators (CPG) based on rhythmic Rowat-Selverston neurons endowed with neuronal and synaptic Hebbian plasticity. To demonstrate the interest of the proposed model, the case of handshaking is considered because it is a very common, both physically and socially, but also, a very complex act in the point of view of robotics, neuroscience and psychology. Plastic CPGs controllers are implemented in the joints of a simulated robotic arm that has to learn the frequency and amplitude of an external force applied to its effector, thus reproducing the act of handshaking with a human. Results show that the neural and synaptic Hebbian plasticity are working together leading to a natural and autonomous synchronization between the arm and the external force even if the frequency is changing during the movement. Moreover, a power consumption analysis shows that, by offering emergence of synchronized and coordinated movements, the plasticity mechanisms lead to a significant decrease in the energy spend by the robot actuators thus generating a more adaptive and natural human/robot handshake.</p>}
}
@inproceedings{hebbian-meta-learning,
	title        = {Meta-learning through hebbian plasticity in random networks},
	author       = {Najarro, Elias and Risi, Sebastian},
	year         = 2020,
	booktitle    = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	location     = {Vancouver, BC, Canada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS '20},
	isbn         = 9781713829546,
	abstract     = {Lifelong learning and adaptability are two defining aspects of biological agents. Modern reinforcement learning (RL) approaches have shown significant progress in solving complex tasks, however once training is concluded, the found solutions are typically static and incapable of adapting to new information or perturbations. While it is still not completely understood how biological brains learn and adapt so efficiently from experience, it is believed that synaptic plasticity plays a prominent role in this process. Inspired by this biological mechanism, we propose a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent. We demonstrate our approach on several reinforcement learning tasks with different sensory modalities and more than 450K trainable plasticity parameters. We find that starting from completely random weights, the discovered Hebbian rules enable an agent to navigate a dynamical 2D-pixel environment; likewise they allow a simulated 3D quadrupedal robot to learn how to walk while adapting to morphological damage not seen during training and in the absence of any explicit reward or error signal in less than 100 timesteps.},
	articleno    = 1740,
	numpages     = 13
}
@inproceedings{henderson2018deep,
	title        = {Deep reinforcement learning that matters},
	author       = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	year         = 2018,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 32,
	number       = 1
}
@article{Hinton2015DistillingTK,
	title        = {Distilling the Knowledge in a Neural Network},
	author       = {Geoffrey E. Hinton and Oriol Vinyals and J. Dean},
	year         = 2015,
	journal      = {ArXiv},
	volume       = {abs/1503.02531}
}
@inproceedings{Humphrys1996ActionSM,
	title        = {Action Selection methods using Reinforcement Learning},
	author       = {Mark Humphrys},
	year         = 1996,
	url          = {https://api.semanticscholar.org/CorpusID:54121911}
}
@article{Hwangbo2017ControlOA,
	title        = {Control of a Quadrotor With Reinforcement Learning},
	author       = {Jemin Hwangbo and Inkyu Sa and Roland Siegwart and Marco Hutter},
	year         = 2017,
	journal      = {IEEE Robotics and Automation Letters},
	volume       = 2,
	pages        = {2096--2103}
}
% This file was created with JabRef 2.10.
% Encoding: UTF-8
@ieeetranbstctl{IEEEexample:BSTcontrol,
	ctldash_repeated_names = {no}
}
@inproceedings{Ilyas2020A,
	title        = {A Closer Look at Deep Policy Gradients},
	author       = {Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations}
}
@article{islam2017reproducibility,
	title        = {Reproducibility of benchmarked deep reinforcement learning tasks for continuous control},
	author       = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1708.04133}
}
@article{juliani2018unity,
	title        = {Unity: A general platform for intelligent agents},
	author       = {Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and others},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1809.02627}
}
@article{KANs,
	title        = {KAN: Kolmogorov-Arnold Networks},
	author       = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v{c}}i{\'c}, Marin and Hou, Thomas Y and Tegmark, Max},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2404.19756}
}
@phdthesis{kearns89,
	title        = {Computational Complexity of Machine Learning},
	author       = {M. J. Kearns},
	year         = 1989,
	school       = {Department of Computer Science, Harvard University}
}
@inproceedings{langley00,
	title        = {Crafting Papers on Machine Learning},
	author       = {P. Langley},
	year         = 2000,
	booktitle    = {Proceedings of the 17th International Conference on Machine Learning (ICML 2000)},
	publisher    = {Morgan Kaufmann},
	address      = {Stanford, CA},
	pages        = {1207--1216},
	editor       = {Pat Langley}
}
@inproceedings{learning2drive,
	title        = {Learning to drive in a day},
	author       = {Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John-Mark and Lam, Vinh-Dieu and Bewley, Alex and Shah, Amar},
	year         = 2019,
	booktitle    = {2019 International Conference on Robotics and Automation (ICRA)},
	pages        = {8248--8254},
	organization = {IEEE}
}
@article{lechner2020neural,
	title        = {Neural circuit policies enabling auditable autonomy},
	author       = {Lechner, Mathias and Hasani, Ramin and Amini, Alexander and Henzinger, Thomas A and Rus, Daniela and Grosu, Radu},
	year         = 2020,
	journal      = {Nature Machine Intelligence},
	publisher    = {Nature Publishing Group UK London},
	volume       = 2,
	number       = 10,
	pages        = {642--652}
}
@article{Lewis2019HowMD,
	title        = {How Much Do Unstated Problem Constraints Limit Deep Robotic Reinforcement Learning?},
	author       = {W Lewis and Mark Moll and Lydia E. Kavraki},
	year         = 2019,
	journal      = {ArXiv},
	volume       = {abs/1909.09282},
	url          = {https://api.semanticscholar.org/CorpusID:202712571}
}
@article{li2017deep,
	title        = {Deep Reinforcement Learning: An Overview},
	author       = {Yuxi Li},
	year         = 2017,
	journal      = {ArXiv},
	volume       = {abs/1701.07274},
	url          = {https://api.semanticscholar.org/CorpusID:17540505}
}
@article{Lillicrap2016ContinuousCW,
	title        = {Continuous control with deep reinforcement learning},
	author       = {T. Lillicrap and J. Hunt and A. Pritzel and N. Heess and T. Erez and Y. Tassa and D. Silver and Daan Wierstra},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1509.02971}
}
@inproceedings{Limitations_of_Scalarisation,
	title        = {On the Limitations of Scalarisation for Multi-objective Reinforcement Learning of Pareto Fronts},
	author       = {Vamplew, Peter and Yearwood, John and Dazeley, Richard and Berry, Adam},
	year         = 2008,
	booktitle    = {AI 2008: Advances in Artificial Intelligence},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {372--378},
	isbn         = {978-3-540-89378-3},
	editor       = {Wobcke, Wayne and Zhang, Mengjie}
}
@inproceedings{linear_weights_comp_1,
	title        = {Dynamic preferences in multi-criteria reinforcement learning},
	author       = {Natarajan, Sriraam and Tadepalli, Prasad},
	year         = 2005,
	booktitle    = {Proceedings of the 22nd international conference on Machine learning},
	pages        = {601--608}
}
@inproceedings{linear_weights_comp_2,
	title        = {Reinforcement learning in the operational management of a water system},
	author       = {Castelletti, Andrea and Corani, Giorgio and Rizzolli, A and Soncinie-Sessa, Rodolfo and Weber, Enrico},
	year         = 2002,
	booktitle    = {IFAC workshop on modeling and control in environmental issues},
	pages        = {325--330},
	organization = {Keio University Yokohama}
}
@misc{liu2019regularization,
	title        = {Regularization Matters in Policy Optimization},
	author       = {Zhuang Liu and Xuanlin Li and Bingyi Kang and Trevor Darrell},
	year         = 2019,
	eprint       = {1910.09191},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{ltc2018,
	title        = {Liquid Time-constant Recurrent Neural Networks as Universal Approximators},
	author       = {Ramin M. Hasani and Mathias Lechner and Alexander Amini and Daniela Rus and Radu Grosu},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1811.00321},
	url          = {http://arxiv.org/abs/1811.00321},
	eprinttype   = {arXiv},
	eprint       = {1811.00321},
	timestamp    = {Thu, 22 Nov 2018 17:58:30 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1811-00321.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{ma2023distillation,
	title        = {Distillation Policy Optimization},
	author       = {Ma, Jianfei},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2302.00533}
}
@article{mabsout2023swannflight,
	title        = {The SwaNNFlight System: On-the-Fly Sim-to-Real Adaptation via Anchored Learning},
	author       = {Mabsout, Bassel El and Roozkhosh, Shahin and Mysore, Siddharth and Saenko, Kate and Mancuso, Renato},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2301.06987}
}
@book{MachineLearningI,
	title        = {Machine Learning: An Artificial Intelligence Approach, Vol. I},
	year         = 1983,
	publisher    = {Tioga},
	address      = {Palo Alto, CA},
	editor       = {R. S. Michalski and J. G. Carbonell and T. M. Mitchell}
}
@misc{mambelli2022compositional,
	title        = {Compositional Multi-Object Reinforcement Learning with Linear Relation Networks},
	author       = {Davide Mambelli and Frederik Träuble and Stefan Bauer and Bernhard Schölkopf and Francesco Locatello},
	year         = 2022,
	eprint       = {2201.13388},
	archiveprefix = {arXiv},
	primaryclass = {cs.RO}
}
@misc{Mateksys,
	title        = {FLIGHT CONTROLLER F722-STD},
	author       = {Mateksys},
	note         = {Accessed: 2021-02-04},
	howpublished = {\url{http://www.mateksys.com/?portfolio=f722-std}}
}
@article{Messac2002GeneratingWS,
	title        = {Generating Well-Distributed Sets of Pareto Points for Engineering Design Using Physical Programming},
	author       = {Achille Messac and Christopher A. Mattson},
	year         = 2002,
	journal      = {Optimization and Engineering},
	volume       = 3,
	pages        = {431--450},
	url          = {https://api.semanticscholar.org/CorpusID:117049923}
}
@inproceedings{MetaSimToReal,
	title        = {Meta Reinforcement Learning for Sim-to-real Domain Adaptation},
	author       = {Arndt, Karol and Hazara, Murtaza and Ghadirzadeh, Ali and Kyrki, Ville},
	year         = 2020,
	booktitle    = {IEEE ICRA},
	volume       = {},
	number       = {},
	pages        = {2725--2731},
	doi          = {10.1109/ICRA40945.2020.9196540}
}
@techreport{mitchell80,
	title        = {The Need for Biases in Learning Generalizations},
	author       = {T. M. Mitchell},
	year         = 1980,
	address      = {New Brunswick, MA},
	institution  = {Computer Science Department, Rutgers University}
}
@article{miyato2018spectral,
	title        = {Spectral normalization for generative adversarial networks},
	author       = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1802.05957}
}
@article{mnih2013playing,
	title        = {Playing atari with deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1312.5602}
}
@article{mnih2015human,
	title        = {Human-level control through deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	year         = 2015,
	journal      = {nature},
	publisher    = {Nature Publishing Group},
	volume       = 518,
	number       = 7540,
	pages        = {529--533}
}
@misc{model_quad_adapt,
	title        = {Modeling and Adaptive Control of a Quadrotor},
	author       = {Schreier, Matthias},
	year         = 2012,
	month        = {08},
	pages        = {},
	doi          = {10.1109/ICMA.2012.6282874}
}
@article{MODRL_framework,
	title        = {A multi-objective deep reinforcement learning framework},
	author       = {Thanh Thi Nguyen and Ngoc Duy Nguyen and Peter Vamplew and Saeid Nahavandi and Richard Dazeley and Chee Peng Lim},
	year         = 2020,
	journal      = {Engineering Applications of Artificial Intelligence},
	volume       = 96,
	pages        = 103915,
	doi          = {https://doi.org/10.1016/j.engappai.2020.103915},
	issn         = {0952-1976},
	url          = {https://www.sciencedirect.com/science/article/pii/S0952197620302475},
	keywords     = {Reinforcement learning, Multi-objective, Deep learning, Single-policy, Multi-policy}
}
@inproceedings{MOMARL,
	title        = {A novel adaptive weight selection algorithm for multi-objective multi-agent reinforcement learning},
	author       = {Van Moffaert, Kristof and Brys, Tim and Chandra, Arjun and Esterle, Lukas and Lewis, Peter R. and Nowé, Ann},
	year         = 2014,
	booktitle    = {2014 International Joint Conference on Neural Networks (IJCNN)},
	volume       = {},
	number       = {},
	pages        = {2306--2314},
	doi          = {10.1109/IJCNN.2014.6889637},
	keywords     = {Cameras;Learning (artificial intelligence);Space exploration;Optimization;Smart cameras;Algorithm design and analysis;Search problems}
}
@article{multiplicative_better,
	title        = {Deep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization},
	author       = {Acero, Fernando and Zehtabi, Parisa and Marchesotti, Nicolas and Cashmore, Michael and Magazzeni, Daniele and Veloso, Manuela},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2403.16667}
}
@article{multiplicative_composition,
	title        = {How to Train Your Quadrotor: A Framework for Consistently Smooth and Responsive Flight Control via Reinforcement Learning},
	author       = {Mysore, Siddharth and Mabsout, Bassel and Saenko, Kate and Mancuso, Renato},
	year         = 2021,
	month        = {sep},
	journal      = {ACM Trans. Cyber-Phys. Syst.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3466618},
	issn         = {2378-962X},
	url          = {https://doi.org/10.1145/3466618},
	issue_date   = {October 2021},
	abstract     = {We focus on the problem of reliably training Reinforcement Learning (RL) models (agents) for stable low-level control in embedded systems and test our methods on a high-performance, custom-built quadrotor platform. A common but often under-studied problem in developing RL agents for continuous control is that the control policies developed are not always smooth. This lack of smoothness can be a major problem when learning controllers as it can result in control instability and hardware failure.Issues of noisy control are further accentuated when training RL agents in simulation due to simulators ultimately being imperfect representations of reality—what is known as the reality gap. To combat issues of instability in RL agents, we propose a systematic framework, REinforcement-based transferable Agents through Learning (RE+AL), for designing simulated training environments that preserve the quality of trained agents when transferred to real platforms. RE+AL is an evolution of the Neuroflight infrastructure detailed in technical reports prepared by members of our research group. Neuroflight is a state-of-the-art framework for training RL agents for low-level attitude control. RE+AL improves and completes Neuroflight by solving a number of important limitations that hindered the deployment of Neuroflight to real hardware. We benchmark RE+AL on the NF1 racing quadrotor developed as part of Neuroflight. We demonstrate that RE+AL significantly mitigates the previously observed issues of smoothness in RL agents. Additionally, RE+AL is shown to consistently train agents that are flight capable and with minimal degradation in controller quality upon transfer. RE+AL agents also learn to perform better than a tuned PID controller, with better tracking errors, smoother control, and reduced power consumption. To the best of our knowledge, RE+AL agents are the first RL-based controllers trained in simulation to outperform a well-tuned PID controller on a real-world controls problem that is solvable with classical control.},
	articleno    = 36,
	numpages     = 24,
	keywords     = {quadrotor, continuous control, Neural networks}
}
@article{Muratore2022,
	title        = {Robot Learning From Randomized Simulations: A Review},
	author       = {Muratore,  Fabio and Ramos,  Fabio and Turk,  Greg and Yu,  Wenhao and Gienger,  Michael and Peters,  Jan},
	year         = 2022,
	month        = apr,
	journal      = {Frontiers in Robotics and AI},
	publisher    = {Frontiers Media SA},
	volume       = 9,
	doi          = {10.3389/frobt.2022.799893},
	issn         = {2296-9144},
	url          = {http://dx.doi.org/10.3389/frobt.2022.799893}
}
@misc{mysore2020regularizing,
	title        = {Regularizing Action Policies for Smooth Control with Reinforcement Learning},
	author       = {Siddharth Mysore and Bassel Mabsout and Renato Mancuso and Kate Saenko},
	year         = 2020,
	eprint       = {2012.06644},
	archiveprefix = {arXiv},
	primaryclass = {cs.RO}
}
@misc{mysore2020train,
	title        = {How to Train your Quadrotor: A Framework for Consistently Smooth and Responsive Flight Control via Reinforcement Learning},
	author       = {Siddharth Mysore and Bassel Mabsout and Kate Saenko and Renato Mancuso},
	year         = 2020,
	eprint       = {2012.06656},
	archiveprefix = {arXiv},
	primaryclass = {cs.RO}
}
@inproceedings{mysore2021caps,
	title        = {Regularizing Action Policies for Smooth Control with Reinforcement Learning},
	author       = {Mysore, Siddharth* and Mabsout, Bassel* and Mancuso, Renato and Saenko, Kate},
	year         = 2021,
	booktitle    = {IEEE ICRA},
	volume       = {},
	number       = {},
	pages        = {1810--1816},
	doi          = {10.1109/ICRA48506.2021.9561138}
}
@article{mysore2021train,
	title        = {How to train your quadrotor: A framework for consistently smooth and responsive flight control via reinforcement learning},
	author       = {Mysore, Siddharth* and Mabsout, Bassel* and Saenko, Kate and Mancuso, Renato},
	year         = 2021,
	journal      = {ACM TCPS},
	publisher    = {ACM New York, NY},
	volume       = 5,
	number       = 4,
	pages        = {1--24}
}
@inproceedings{mysore2022multicritic,
	title        = {Multi-Critic Actor Learning: Teaching {RL} Policies to Act with Style},
	author       = {Mysore, Siddharth and Cheng, George and Zhao, Yunqi and Saenko, Kate and Wu, Meng},
	year         = 2022,
	booktitle    = {ICLR},
	url          = {https://openreview.net/forum?id=rJvY_5OzoI},
	note         = {accepted}
}
@article{Nagabandi2019LearningTA,
	title        = {Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning},
	author       = {Anusha Nagabandi and Ignasi Clavera and Simin Liu and Ronald S. Fearing and P. Abbeel and Sergey Levine and Chelsea Finn},
	year         = 2019,
	journal      = {arXiv: Learning}
}
@article{naik2024reward,
	title        = {Reward Centering},
	author       = {Naik, Abhishek and Wan, Yi and Tomar, Manan and Sutton, Richard S},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2405.09999}
}
@inproceedings{NEURIPS2018_a2802cad,
	title        = {Diversity-Driven Exploration Strategy for Deep Reinforcement Learning},
	author       = {Hong, Zhang-Wei and Shann, Tzu-Yun and Su, Shih-Yang and Chang, Yi-Hsiang and Fu, Tsu-Jui and Lee, Chun-Yi},
	year         = 2018,
	booktitle    = {NeurIPS},
	publisher    = {Curran Associates, Inc.},
	volume       = 31,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2018/file/a2802cade04644083dcde1c8c483ed9a-Paper.pdf},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@incollection{Newell81,
	title        = {Mechanisms of Skill Acquisition and the Law of Practice},
	author       = {A. Newell and P. S. Rosenbloom},
	year         = 1981,
	booktitle    = {Cognitive Skills and Their Acquisition},
	publisher    = {Lawrence Erlbaum Associates, Inc.},
	address      = {Hillsdale, NJ},
	pages        = {1--51},
	editor       = {J. R. Anderson},
	chapter      = 1
}
@article{NFori,
	title        = {Reinforcement Learning for UAV Attitude Control},
	author       = {Koch, William and Mancuso, Renato and West, Richard and Bestavros, Azer},
	year         = 2019,
	journal      = {ACM Transactions on Cyber-Physical Systems},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3301273},
	issn         = {2378-962X},
	url          = {https://doi.org/10.1145/3301273},
	issue_date   = {March 2019},
	articleno    = 22,
	keywords     = {Attitude control, adaptive control, machine learning, UAV, intelligent control, autopilot, reinforcement learning, quadcopter, PID}
}
@phdthesis{NFThesis,
	title        = {Flight Controller Synthesis Via Deep Reinforcement Learning},
	author       = {William Koch},
	year         = 2019,
	school       = {Department of Computer Science, Boston University},
	eprint       = {1909.06493},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{NFv2,
	title        = {Neuroflight: Next Generation Flight Control Firmware},
	author       = {William Koch and Renato Mancuso and Azer Bestavros},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1901.06553},
	url          = {http://arxiv.org/abs/1901.06553},
	archiveprefix = {arXiv},
	eprint       = {1901.06553},
	timestamp    = {Fri, 01 Feb 2019 13:39:59 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1901-06553.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{NIPS2006_98c39996,
	title        = {An Application of Reinforcement Learning to Aerobatic Helicopter Flight},
	author       = {Abbeel, Pieter and Coates, Adam and Quigley, Morgan and Ng, Andrew},
	year         = 2006,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {MIT Press},
	volume       = 19,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2006/file/98c39996bf1543e974747a2549b3107c-Paper.pdf},
	editor       = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman}
}
@misc{noise,
	title        = {Python Noise library version 1.2.3},
	url          = {https://github.com/caseman/noise/tree/bb32991ab97e90882d0e46e578060717c5b90dc5},
	note         = {Accessed: 2021-02-04}
}
@article{non_linear_comp,
	title        = {Managing power consumption and performance of computing systems using reinforcement learning},
	author       = {Tesauro, Gerald and Das, Rajarshi and Chan, Hoi and Kephart, Jeffrey and Levine, David and Rawson, Freeman and Lefurgy, Charles},
	year         = 2007,
	journal      = {Advances in neural information processing systems},
	volume       = 20
}
@misc{ODE,
	title        = {Open Dynamics Engine},
	journal      = {Open Dynamics Engine},
	url          = {https://www.ode.org/}
}
@misc{openai2018learning,
	title        = {Learning Dexterous In-Hand Manipulation},
	author       = {OpenAI and Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Jozefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
	year         = 2018,
	eprint       = {1808.00177},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{openai5,
	title        = {Dota 2 with Large Scale Deep Reinforcement Learning},
	author       = {Christopher Berner and others},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1912.06680},
	archiveprefix = {arXiv},
	eprint       = {1912.06680}
}
@article{orevi2023,
	title        = {Computation offloading for ground robotic systems communicating over WiFi - an empirical exploration on performance and energy trade-offs},
	author       = {Dordevic,  Milica and Albonico,  Michel and Lewis,  Grace A. and Malavolta,  Ivano and Lago,  Patricia},
	year         = 2023,
	month        = oct,
	journal      = {Empirical Software Engineering},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 28,
	number       = 6,
	doi          = {10.1007/s10664-023-10351-6},
	issn         = {1573-7616},
	url          = {http://dx.doi.org/10.1007/s10664-023-10351-6}
}
@article{Overfitting,
	title        = {A Study on Overfitting in Deep Reinforcement Learning},
	author       = {Chiyuan Zhang and Oriol Vinyals and R{\'e}mi Munos and Samy Bengio},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1804.06893}
}
@article{Overfitting2,
	title        = {A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning},
	author       = {Amy X. Zhang and Nicolas Ballas and Joelle Pineau},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1806.07937}
}
@article{pareto_q_learning,
	title        = {Multi-Objective Reinforcement Learning using Sets of Pareto Dominating Policies},
	author       = {Kristof Van Moffaert and Ann Now{{\'e}}},
	year         = 2014,
	journal      = {Journal of Machine Learning Research},
	volume       = 15,
	number       = 107,
	pages        = {3663--3692},
	url          = {http://jmlr.org/papers/v15/vanmoffaert14a.html}
}
@article{PasikDuncan1996AdaptiveC,
	title        = {Adaptive Control},
	author       = {Bozenna Pasik-Duncan},
	year         = 1996,
	journal      = {IEEE Control Syst.},
	volume       = 16,
	pages        = {87-}
}
@article{perera2021applications,
	title        = {Applications of reinforcement learning in energy systems},
	author       = {Perera, ATD and Kamalaruban, Parameswaran},
	year         = 2021,
	journal      = {Renewable and Sustainable Energy Reviews},
	publisher    = {Elsevier},
	volume       = 137,
	pages        = 110618
}
@article{pianosi2013tree,
	title        = {Tree-based fitted Q-iteration for multi-objective Markov decision processes in water resource management},
	author       = {Pianosi, Francesca and Castelletti, Andrea and Restelli, Marcello},
	year         = 2013,
	journal      = {Journal of Hydroinformatics},
	publisher    = {IWA Publishing},
	volume       = 15,
	number       = 2,
	pages        = {258--270}
}
@misc{pineau2020improving,
	title        = {Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)},
	author       = {Joelle Pineau and Philippe Vincent-Lamarre and Koustuv Sinha and Vincent Larivière and Alina Beygelzimer and Florence d'Alché-Buc and Emily Fox and Hugo Larochelle},
	year         = 2020,
	eprint       = {2003.12206},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{pinto2017asymmetric,
	title        = {Asymmetric actor critic for image-based robot learning},
	author       = {Pinto, Lerrel and Andrychowicz, Marcin and Welinder, Peter and Zaremba, Wojciech and Abbeel, Pieter},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1710.06542}
}
@inproceedings{pmlr-v155-sandha21a,
	title        = {Sim2Real Transfer for Deep Reinforcement Learning with Stochastic State Transition Delays},
	author       = {Sandha, Sandeep Singh and Garcia, Luis and Balaji, Bharathan and Anwar, Fatima and Srivastava, Mani},
	year         = 2021,
	month        = {16--18 Nov},
	booktitle    = {Proceedings of the 2020 Conference on Robot Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 155,
	pages        = {1066--1083},
	url          = {https://proceedings.mlr.press/v155/sandha21a.html},
	editor       = {Kober, Jens and Ramos, Fabio and Tomlin, Claire},
	pdf          = {https://proceedings.mlr.press/v155/sandha21a/sandha21a.pdf},
	abstract     = {Deep Reinforcement Learning (RL) has demonstrated to be useful for a wide variety of robotics applications. To address sample efficiency and safety during training, it is common to train Deep RL policies in a simulator and then deploy to the real world, a process called Sim2Real transfer.  For robotics applications, the deployment heterogeneities and runtime compute stochasticity results in variable timing characteristics of sensor sampling rates and end-to-end delays from sensing to actuation. Prior works have used the technique of domain randomization to enable the successful transfer of policies across domains having different state transition delays. We show that variation in sampling rates and policy execution time leads to degradation in Deep RL policy performance, and that domain randomization is insufficient to overcome this limitation. We propose the Time-in-State RL (TSRL) approach, which includes delays and sampling rate as additional agent observations at training time to improve the robustness of Deep RL policies. We demonstrate the efficacy of TSRL on HalfCheetah, Ant, and car robot in simulation and on a real robot using a 1/18th scale car.}
}
@article{pohlen2018observe,
	title        = {Observe and Look Further: Achieving Consistent Performance on Atari. arXiv e-prints},
	author       = {Pohlen, T and Piot, B and Hester, T and Gheshlaghi Azar, M and Horgan, D and Budden, D and Barth-Maron, G and van Hasselt, H and Quan, J and Vecer{\i}k, M and others},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1805.11593}
}
@article{PPO,
	title        = {Proximal Policy Optimization Algorithms},
	author       = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1707.06347},
	archiveprefix = {arXiv},
	eprint       = {1707.06347},
	timestamp    = {Mon, 13 Aug 2018 16:47:34 +0200},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{practical_guide,
	title        = {A practical guide to multi-objective reinforcement learning and planning},
	author       = {Hayes, Conor and Rădulescu, Roxana and Bargiacchi, Eugenio and Källström, Johan and Macfarlane, Matthew and Reymond, Mathieu and Verstraeten, Timothy and Zintgraf, Luisa and Dazeley, Richard and Heintz, Fredrik and Howley, Enda and Irissappane, Athirai and Mannion, Patrick and Nowe, Ann and Ramos, Gabriel and Restelli, Marcello and Vamplew, Peter and Roijers, Diederik},
	year         = 2022,
	month        = {04},
	journal      = {Autonomous Agents and Multi-Agent Systems},
	volume       = 36,
	pages        = {},
	doi          = {10.1007/s10458-022-09552-y}
}
@misc{pybullet,
	title        = {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
	author       = {Erwin Coumans and Yunfei Bai},
	year         = {2016--2019},
	howpublished = {\url{http://pybullet.org}}
}
@article{q_managed,
	title        = {Q-Managed: A new algorithm for a multiobjective reinforcement learning},
	author       = {Thiago Henrique Freire de Oliveira and Luiz Paulo de Souza Medeiros and Adrião Duarte Dória Neto and Jorge Dantas Melo},
	year         = 2021,
	journal      = {Expert Systems with Applications},
	volume       = 168,
	pages        = 114228,
	doi          = {https://doi.org/10.1016/j.eswa.2020.114228},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S0957417420309490},
	keywords     = {Multiobjective reinforcement learning, ε-constraint, Q-Learning, Pareto dominance, Single-policy approach, Hypervolume}
}
@article{Qlearning,
	title        = {Q-learning},
	author       = {Watkins, Christopher JCH and Dayan, Peter},
	year         = 1992,
	journal      = {Machine learning},
	publisher    = {Springer},
	volume       = 8,
	number       = {3-4},
	pages        = {279--292}
}
@misc{Quadrotor,
	title        = {Quadrotor},
	howpublished = {\url{https://rotorbuilds.com/build/15163}}
}
@article{radiotherapy,
	title        = {Multi-objective optimization of radiotherapy: distributed Q-learning and agent-based simulation},
	author       = {Ammar Jalalimanesh and Hamidreza Shahabi Haghighi and Abbas Ahmadi and Hossein Hejazian and Madjid Soltani},
	year         = 2017,
	journal      = {Journal of Experimental \& Theoretical Artificial Intelligence},
	publisher    = {Taylor \& Francis},
	volume       = 29,
	number       = 5,
	pages        = {1071--1086},
	doi          = {10.1080/0952813X.2017.1292319},
	url          = {https://doi.org/10.1080/0952813X.2017.1292319}
}
@article{RAZZAGHI2024108911,
	title        = {A survey on reinforcement learning in aviation applications},
	author       = {Pouria Razzaghi and Amin Tabrizian and Wei Guo and Shulu Chen and Abenezer Taye and Ellis Thompson and Alexis Bregeon and Ali Baheri and Peng Wei},
	year         = 2024,
	journal      = {Engineering Applications of Artificial Intelligence},
	volume       = 136,
	pages        = 108911,
	doi          = {https://doi.org/10.1016/j.engappai.2024.108911},
	issn         = {0952-1976},
	url          = {https://www.sciencedirect.com/science/article/pii/S0952197624010698},
	keywords     = {Reinforcement learning, Deep reinforcement learning, Aviation, Aircraft, Machine learning, Artificial intelligence}
}
@article{REINFORCE,
	title        = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	author       = {Williams, Ronald J},
	year         = 1992,
	journal      = {Machine learning},
	publisher    = {Springer},
	volume       = 8,
	number       = {3-4},
	pages        = {229--256}
}
@inproceedings{repRL,
	title        = {State Representation Learning in Robotics: Using Prior Knowledge about Physical Interaction.},
	author       = {Jonschkowski, Rico and Brock, Oliver},
	year         = 2014,
	booktitle    = {Robotics: Science and Systems}
}
@article{repRL2,
	title        = {Learning state representations with robotic priors},
	author       = {Jonschkowski, Rico and Brock, Oliver},
	year         = 2015,
	journal      = {Autonomous Robots},
	publisher    = {Springer},
	volume       = 39,
	number       = 3,
	pages        = {407--428}
}
@article{reward_interpolation,
	title        = {Predicting optimal value functions by interpolating reward functions in scalarized multi-objective reinforcement learning},
	author       = {Arpan Kusari and Jonathan P. How},
	year         = 2019,
	journal      = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
	pages        = {7484--7490},
	url          = {https://api.semanticscholar.org/CorpusID:202558786}
}
@article{reward_is_enough,
	title        = {Reward is enough},
	author       = {David Silver and Satinder Singh and Doina Precup and Richard S. Sutton},
	year         = 2021,
	journal      = {Artificial Intelligence},
	volume       = 299,
	pages        = 103535,
	doi          = {https://doi.org/10.1016/j.artint.2021.103535},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370221000862},
	keywords     = {Artificial intelligence, Artificial general intelligence, Reinforcement learning, Reward}
}
@article{RL_algo_survey,
	title        = {Reinforcement learning algorithms: A brief survey},
	author       = {Ashish Kumar Shakya and Gopinatha Pillai and Sohom Chakrabarty},
	year         = 2023,
	journal      = {Expert Systems with Applications},
	volume       = 231,
	pages        = 120495,
	doi          = {https://doi.org/10.1016/j.eswa.2023.120495},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S0957417423009971},
	keywords     = {Reinforcement learning, Stochastic optimal control, Function approximation, Deep Reinforcement Learning (DRL)}
}
@inproceedings{RL_challenges,
	title        = {Challenges of Real-World Reinforcement Learning},
	author       = {Daniel J. Mankowitz and Gabriel Dulac-Arnold and Todd Hester},
	year         = 2019
}
@article{RL_intro_apps,
	title        = {A Gentle Introduction to Reinforcement Learning and its Application in Different Fields},
	author       = {Naeem, Muddasar and Rizvi, Syed Tahir Hussain and Coronato, Antonio},
	year         = 2020,
	journal      = {IEEE Access},
	volume       = 8,
	number       = {},
	pages        = {209320--209344},
	doi          = {10.1109/ACCESS.2020.3038605},
	keywords     = {Mathematical model;Prediction algorithms;Heuristic algorithms;Robot kinematics;Reinforcement learning;Markov processes;Task analysis;Artificial intelligence;reinforcement learning;applications;healthcare;robotics;communication;natural language processing;computer vision;resource management;IoT}
}
@misc{rl-zoo,
	title        = {RL Hyperparameter Zoo},
	author       = {},
	year         = 2018,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/araffin/rl-baselines-zoo/tree/master/hyperparams}}
}
@inproceedings{RoboImitationPeng20,
	title        = {Learning Agile Robotic Locomotion Skills by Imitating Animals},
	author       = {Peng, Xue Bin and Coumans, Erwin and Zhang, Tingnan and Lee, Tsang-Wei Edward and Tan, Jie and Levine, Sergey},
	year         = 2020,
	month        = {07},
	booktitle    = {Robotics: Science and Systems},
	doi          = {10.15607/RSS.2020.XVI.064}
}
@article{rusu2015policy,
	title        = {Policy distillation},
	author       = {Rusu, Andrei A and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
	year         = 2015,
	journal      = {arXiv preprint arXiv:1511.06295}
}
@article{SAC,
	title        = {Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
	author       = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	year         = 2018,
	journal      = {International Conference on Machine Learning (ICML)}
}
@inproceedings{safety_gym,
	title        = {Safety Gymnasium: A Unified Safe Reinforcement Learning Benchmark},
	author       = {Jiaming Ji and Borong Zhang and Jiayi Zhou and Xuehai Pan and Weidong Huang and Ruiyang Sun and Yiran Geng and Yifan Zhong and Josef Dai and Yaodong Yang},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
	url          = {https://openreview.net/forum?id=WZmlxIuIGR}
}
@article{SAKAWA199819,
	title        = {Interactive decision-making for multiobjective linear fractional programming problems with block angular structure involving fuzzy numbers},
	author       = {Masatoshi Sakawa and Kosuke Kato},
	year         = 1998,
	journal      = {Fuzzy Sets and Systems},
	volume       = 97,
	number       = 1,
	pages        = {19--31},
	doi          = {https://doi.org/10.1016/S0165-0114(96)00352-1},
	issn         = {0165-0114},
	url          = {https://www.sciencedirect.com/science/article/pii/S0165011496003521},
	keywords     = {Multiobjective linear fractional programming, Block angular structure, Fuzzy numbers, Interactive decision making}
}
@article{SAKAWA1998564,
	title        = {An interactive fuzzy satisficing method for multiobjective 0–1 programming problems with fuzzy numbers through genetic algorithms with double strings},
	author       = {Masatoshi Sakawa and Toshihiro Shibano},
	year         = 1998,
	journal      = {European Journal of Operational Research},
	volume       = 107,
	number       = 3,
	pages        = {564--574},
	doi          = {https://doi.org/10.1016/S0377-2217(97)00197-5},
	issn         = {0377-2217},
	url          = {https://www.sciencedirect.com/science/article/pii/S0377221797001975},
	keywords     = {Multiobjective 0–1 programming, Fuzzy numbers, Fuzzy goals, α-Pareto optimal, Interactive methods Genetic algorithms}
}
@article{Samuel59,
	title        = {Some Studies in Machine Learning Using the Game of Checkers},
	author       = {A. L. Samuel},
	year         = 1959,
	journal      = {IBM Journal of Research and Development},
	volume       = 3,
	number       = 3,
	pages        = {211--229}
}
@book{SARSA,
	title        = {On-line Q-learning using connectionist systems},
	author       = {Rummery, Gavin A and Niranjan, Mahesan},
	year         = 1994,
	volume       = 37
}
@article{scaman2018lipschitz,
	title        = {Lipschitz regularity of deep neural networks: analysis and efficient estimation},
	author       = {Scaman, Kevin and Virmaux, Aladin},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1805.10965}
}
@article{schaul2021return,
	title        = {Return-based scaling: Yet another normalisation trick for deep rl},
	author       = {Schaul, Tom and Ostrovski, Georg and Kemaev, Iurii and Borsa, Diana},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2105.05347}
}
@article{schrittwieser2020mastering,
	title        = {Mastering atari, go, chess and shogi by planning with a learned model},
	author       = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
	year         = 2020,
	journal      = {Nature},
	publisher    = {Nature Publishing Group UK London},
	volume       = 588,
	number       = 7839,
	pages        = {604--609}
}
@article{self-tuning,
	title        = {A self-tuning actor-critic algorithm},
	author       = {Zahavy, Tom and Xu, Zhongwen and Veeriah, Vivek and Hessel, Matteo and Oh, Junhyuk and van Hasselt, Hado P and Silver, David and Singh, Satinder},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33
}
@inproceedings{shelton2001balancing,
	title        = {Balancing multiple sources of reward in reinforcement learning},
	author       = {Shelton, Christian R},
	year         = 2001,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {1082--1088}
}
@misc{shen2020deep,
	title        = {Deep Reinforcement Learning with Smooth Policy},
	author       = {Qianli Shen and Yan Li and Haoming Jiang and Zhaoran Wang and Tuo Zhao},
	year         = 2020,
	eprint       = {2003.09534},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{silver2016mastering,
	title        = {Mastering the game of Go with deep neural networks and tree search},
	author       = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
	year         = 2016,
	journal      = {nature},
	publisher    = {Nature Publishing Group},
	volume       = 529,
	number       = 7587,
	pages        = {484--489}
}
@article{silver2017mastering,
	title        = {Mastering the game of go without human knowledge},
	author       = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
	year         = 2017,
	journal      = {nature},
	publisher    = {Nature Publishing Group},
	volume       = 550,
	number       = 7676,
	pages        = {354--359}
}
@article{silver2018general,
	title        = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	author       = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
	year         = 2018,
	journal      = {Science},
	publisher    = {American Association for the Advancement of Science},
	volume       = 362,
	number       = 6419,
	pages        = {1140--1144}
}
@article{Sim2multi,
	title        = {Sim-to-(Multi)-Real: Transfer of Low-Level Robust Control Policies to Multiple Quadrotors},
	author       = {Artem Molchanov and Tao Chen and Wolfgang H{\"{o}}nig and James A. Preiss and Nora Ayanian and Gaurav S. Sukhatme},
	year         = 2019,
	journal      = {International Conference on Intelligent Robots and Systems},
	url          = {http://arxiv.org/abs/1903.04628},
	archiveprefix = {arXiv},
	eprint       = {1903.04628},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1903-04628.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Sim2Real,
	title        = {{Sim2Real} View Invariant Visual Servoing by Recurrent Control},
	author       = {Fereshteh Sadeghi and Alexander Toshev and Eric Jang and Sergey Levine},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1712.07642},
	archiveprefix = {arXiv},
	eprint       = {1712.07642},
	timestamp    = {Mon, 13 Aug 2018 16:46:34 +0200},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{simonyan2014,
	title        = {Very deep convolutional networks for large-scale image recognition},
	author       = {Simonyan, Karen and Zisserman, Andrew},
	year         = 2014,
	journal      = {arXiv preprint arXiv:1409.1556}
}
}
@article{sinha2020d2rl,
	title        = {D2RL: Deep Dense Architectures in Reinforcement Learning},
	author       = {Sinha, Samarth and Bharadhwaj, Homanga and Srinivas, Aravind and Garg, Animesh},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.09163}
}
@inproceedings{Song2020ProvablyEM,
	title        = {Provably Efficient Model-based Policy Adaptation},
	author       = {Yuda Song and Aditi Mavalankar and Wen Sun and Sicun Gao},
	year         = 2020,
	booktitle    = {ICML}
}
@article{SpinningUp2018,
	title        = {{Spinning Up in Deep Reinforcement Learning}},
	author       = {Achiam, Joshua},
	year         = 2018
}
@misc{stable-baselines,
	title        = {Stable Baselines},
	author       = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
	year         = 2018,
	journal      = {GitHub repository},
	publisher    = {GitHub}
}
@misc{stable-baselines3,
	title        = {Stable Baselines3},
	author       = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
	year         = 2019,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/DLR-RM/stable-baselines3}}
}
@article{survey_seq_dec_morl,
	title        = {A survey of multi-objective sequential decision-making},
	author       = {Roijers, Diederik M and Vamplew, Peter and Whiteson, Shimon and Dazeley, Richard},
	year         = 2013,
	journal      = {Journal of Artificial Intelligence Research},
	volume       = 48,
	pages        = {67--113}
}
@inproceedings{sutton2000policy,
	title        = {Policy gradient methods for reinforcement learning with function approximation},
	author       = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
	year         = 2000,
	booktitle    = {Advances in neural information processing systems},
	pages        = {1057--1063}
}
@book{sutton2018reinforcement,
	title        = {Reinforcement learning: An introduction},
	author       = {Sutton, Richard S and Barto, Andrew G},
	year         = 2018,
	publisher    = {MIT press}
}
@book{SuttonBarto,
	title        = {Introduction to Reinforcement Learning},
	author       = {Sutton, Richard S. and Barto, Andrew G.},
	year         = 1998,
	publisher    = {MIT Press},
	address      = {Cambridge, MA, USA},
	isbn         = {0262193981},
	edition      = {1st}
}
@misc{swannlake-github,
	title        = {Swappable Neural Network Flight control System Homepage},
	url          = {https://github.com/BU-Cyber-Physical-Systems-Lab/SwaNNLake}
}
@misc{tasfi2016PLE,
	title        = {PyGame Learning Environment},
	author       = {Tasfi, Norman},
	year         = 2016,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/ntasfi/PyGame-Learning-Environment}}
}
@inproceedings{TD3,
	title        = {Addressing Function Approximation Error in Actor-Critic Methods},
	author       = {Fujimoto, Scott and Hoof, Herke and Meger, David},
	year         = 2018,
	booktitle    = {International Conference on Machine Learning},
	pages        = {1587--1596}
}
@misc{TFAgents,
	title        = {{TF-Agents}: A library for Reinforcement Learning in TensorFlow},
	author       = {Sergio Guadarrama and Anoop Korattikara and Oscar Ramirez and Pablo Castro and Ethan Holly and Sam Fishman and Ke Wang and Ekaterina Gonina and Neal Wu and Efi Kokiopoulou and Luciano Sbaiz and Jamie Smith and Gábor Bartók and Jesse Berent and Chris Harris and Vincent Vanhoucke and Eugene Brevdo},
	year         = 2,
	url          = {https://github.com/tensorflow/agents}
}
@inproceedings{todorov2012mujoco,
	title        = {Mujoco: A physics engine for model-based control},
	author       = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	year         = 2012,
	booktitle    = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
	pages        = {5026--5033},
	organization = {IEEE}
}
@article{tokamak,
	title        = {Magnetic control of tokamak plasmas through deep reinforcement learning},
	author       = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and Casas, Diego and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Riedmiller, Martin},
	year         = 2022,
	month        = {02},
	journal      = {Nature},
	volume       = 602,
	pages        = {414--419},
	doi          = {10.1038/s41586-021-04301-9}
}
@inproceedings{TQC,
	title        = {Controlling overestimation bias with truncated mixture of continuous distributional quantile critics},
	author       = {Kuznetsov, Arsenii and Shvechikov, Pavel and Grishin, Alexander and Vetrov, Dmitry},
	year         = 2020,
	booktitle    = {International Conference on Machine Learning},
	pages        = {5556--5566},
	organization = {PMLR}
}
@article{transfPolicies,
	title        = {Learning Transferable Policies for Monocular Reactive {MAV} Control},
	author       = {Shreyansh Daftry and J. Andrew Bagnell and Martial Hebert},
	year         = 2016,
	journal      = {International Symposium on Experimental Robotics},
	archiveprefix = {arXiv},
	eprint       = {1608.00627},
	timestamp    = {Mon, 13 Aug 2018 16:47:31 +0200}
}
@inproceedings{TRPO,
	title        = {Trust Region Policy Optimization},
	author       = {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
	year         = 2015,
	month        = {07--09 Jul},
	booktitle    = {Proceedings of the 32nd International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Lille, France},
	series       = {Proceedings of Machine Learning Research},
	volume       = 37,
	pages        = {1889--1897},
	editor       = {Francis Bach and David Blei},
	abstract     = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}
@article{Ullrich2017SoftWF,
	title        = {Soft Weight-Sharing for Neural Network Compression},
	author       = {Karen Ullrich and Edward Meeds and M. Welling},
	year         = 2017,
	journal      = {ArXiv},
	volume       = {abs/1702.04008}
}
@inproceedings{Vamplew2015,
	title        = {Reinforcement Learning of Pareto-Optimal Multiobjective Policies Using Steering},
	author       = {Vamplew, Peter and Issabekov, Rustam and Dazeley, Richard and Foale, Cameron},
	year         = 2015,
	month        = 11,
	pages        = {},
	doi          = {10.1007/978-3-319-26350-2_53},
	isbn         = {978-3-319-26349-6}
}
@article{Vamplew2017,
	title        = {Softmax exploration strategies for multiobjective reinforcement learning},
	author       = {Peter Vamplew and Richard Dazeley and Cameron Foale},
	year         = 2017,
	journal      = {Neurocomputing},
	volume       = 263,
	pages        = {74--86},
	doi          = {https://doi.org/10.1016/j.neucom.2016.09.141},
	issn         = {0925-2312},
	url          = {https://www.sciencedirect.com/science/article/pii/S0925231217310974},
	note         = {Multiobjective Reinforcement Learning: Theory and Applications},
	keywords     = {Multiobjective reinforcement learning, Exploration, ϵ-greedy exploration, Optimistic initialisation, Softmax}
}
@inproceedings{Van_Moffaert,
	title        = {Scalarized multi-objective reinforcement learning: Novel design techniques},
	author       = {Van Moffaert, Kristof and Drugan, Madalina M. and Nowé, Ann},
	year         = 2013,
	booktitle    = {2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
	volume       = {},
	number       = {},
	pages        = {191--199},
	doi          = {10.1109/ADPRL.2013.6615007},
	keywords     = {Pareto optimization;Learning (artificial intelligence);Chebyshev approximation;Benchmark testing;Measurement;Shape}
}
@article{VERSTRAETEN2019428,
	title        = {Fleetwide data-enabled reliability improvement of wind turbines},
	author       = {Timothy Verstraeten and Ann Nowé and Jonathan Keller and Yi Guo and Shuangwen Sheng and Jan Helsen},
	year         = 2019,
	journal      = {Renewable and Sustainable Energy Reviews},
	volume       = 109,
	pages        = {428--437},
	doi          = {https://doi.org/10.1016/j.rser.2019.03.019},
	issn         = {1364-0321},
	url          = {https://www.sciencedirect.com/science/article/pii/S1364032119301522},
	keywords     = {Wind turbine reliability, Data-enabled load analysis, Failure avoidance}
}
@article{vinyals2019grandmaster,
	title        = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
	author       = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
	year         = 2019,
	journal      = {Nature},
	publisher    = {Nature Publishing Group},
	volume       = 575,
	number       = 7782,
	pages        = {350--354}
}
@article{vorbach2021causal,
	title        = {Causal navigation by continuous-time neural networks},
	author       = {Vorbach, Charles and Hasani, Ramin and Amini, Alexander and Lechner, Mathias and Rus, Daniela},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {12425--12440}
}
@inproceedings{wang2020truly,
	title        = {Truly proximal policy optimization},
	author       = {Wang, Yuhui and He, Hao and Tan, Xiaoyang},
	year         = 2020,
	booktitle    = {Uncertainty in artificial intelligence},
	pages        = {113--122},
	organization = {PMLR}
}
@article{Wingate_Temporal_MORL,
	title        = {Using logical specifications of objectives in multi-objective reinforcement learning},
	author       = {Nottingham, Kolby and Balakrishnan, Anand and Deshmukh, Jyotirmoy and Wingate, David},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1910.01723}
}
@article{winningNotEverything,
	title        = {Winning Is Not Everything: Enhancing Game Development With Intelligent Agents},
	author       = {Y. {Zhao} and I. {Borovikov} and F. {de Mesentier Silva} and A. {Beirami} and J. {Rupert} and C. {Somers} and J. {Harder} and J. {Kolen} and J. {Pinto} and R. {Pourabolghasem} and J. {Pestrak} and H. {Chaput} and M. {Sardari} and L. {Lin} and S. {Narravula} and N. {Aghdaie} and K. {Zaman}},
	year         = 2020,
	journal      = {IEEE Transactions on Games},
	volume       = 12,
	number       = 2,
	doi          = {10.1109/TG.2020.2990865}
}
@article{wu2016drone,
	title        = {Drone streaming with Wi-Fi grid aggregation for virtual tour},
	author       = {Wu, Chenglei and Wang, Zhi and Yang, Shiqiang},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1605.09486}
}
@inproceedings{xu2020prediction,
	title        = {Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control},
	author       = {Xu, Jie and Tian, Yunsheng and Ma, Pingchuan and Rus, Daniela and Sueda, Shinjiro and Matusik, Wojciech},
	year         = 2020,
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning}
}
@misc{yang2019generalized,
	title        = {A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation},
	author       = {Runzhe Yang and Xingyuan Sun and Karthik Narasimhan},
	year         = 2019,
	eprint       = {1908.08342},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{yang2019hierarchical,
	title        = {Hierarchical cooperative multi-agent reinforcement learning with skill discovery},
	author       = {Yang, Jiachen and Borovikov, Igor and Zha, Hongyuan},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1912.03558}
}
@inbook{Yang2023,
	title        = {Reinforcement Learning by Guided Safe Exploration},
	author       = {Yang,  Qisong and Simão,  Thiago D. and Jansen,  Nils and Tindemans,  Simon H. and Spaan,  Matthijs T. J.},
	year         = 2023,
	month        = sep,
	booktitle    = {ECAI 2023},
	publisher    = {IOS Press},
	doi          = {10.3233/faia230598},
	isbn         = 9781643684376,
	issn         = {1879-8314},
	url          = {http://dx.doi.org/10.3233/FAIA230598}
}
@inproceedings{yu2019sim,
	title        = {Sim-to-real transfer for biped locomotion},
	author       = {Yu, Wenhao and Kumar, Visak CV and Turk, Greg and Liu, C Karen},
	year         = 2019,
	booktitle    = {IEEE/RSJ IROS},
	pages        = {3503--3510},
	organization = {IEEE}
}
@article{zhang2024survey,
	title        = {A survey on applications of reinforcement learning in spatial resource allocation},
	author       = {Zhang, Di and Wang, Moyang and Mango, Joseph and Li, Xiang and Xu, Xianrui},
	year         = 2024,
	journal      = {Computational Urban Science},
	publisher    = {Springer},
	volume       = 4,
	number       = 1,
	pages        = 14
}
@article{zhao2019multi,
	title        = {On multi-agent learning in team sports games},
	author       = {Zhao, Yunqi and Borovikov, Igor and Rupert, Jason and Somers, Caedmon and Beirami, Ahmad},
	year         = 2019,
	journal      = {arXiv:1906.10124}
}
@inproceedings{zhao2020sim,
	title        = {Sim-to-real transfer in deep reinforcement learning for robotics: a survey},
	author       = {Zhao, Wenshuai and Queralta, Jorge Pe{\~n}a and Westerlund, Tomi},
	year         = 2020,
	booktitle    = {IEEE SSCI},
	pages        = {737--744},
	organization = {IEEE}
}
@article{Ziegler1942OptimumSF,
	title        = {Optimum Settings for Automatic Controllers},
	author       = {J. G. Ziegler and N. B. Nichols},
	year         = 1942,
	journal      = {Journal of Dynamic Systems Measurement and Control-transactions of The Asme},
	volume       = 115,
	pages        = {220--222}
}
@article{ZieglerNichols,
	title        = {{Optimum Settings for Automatic Controllers}},
	author       = {Ziegler, J. G. and Nichols, N. B.},
	year         = 1993,
	month        = {06},
	journal      = {Journal of Dynamic Systems, Measurement, and Control},
	volume       = 115,
	number       = {2B},
	pages        = {220--222},
	issn         = {0022-0434},
	abstract     = {{In this paper, the three principal control effects found in present controllers are examined and practical names and units of measurement are proposed for each effect. Corresponding units are proposed for a classification of industrial processes in terms of the two principal characteristics affecting their controllability. Formulas are given which enable the controller settings to be determined from the experimental or calculated values of the lag and unit reaction rate of the process to be controlled. These units form the basis of a quick method for adjusting a controller on the job. The effect of varying each controller setting is shown in a series of chart records. It is believed that the conceptions of control presented in this paper will be of assistance in the adjustment of existing controller applications and in the design of new installations.}}
}
